[backend]
provider = "ollama"
url = "http://localhost:11434/api/chat"
# Uncomment to use authorization  
#token = "token"

# Example OpenAi backend config
# provider = "openai"
# url = "https://api.openai.com/v1/chat/completions"
# token = "token"

[llm]
model = "llama3.2"

[nelson]
# ["cmd", "neat", "long", "code"]
default_mode = "cmd"
